

<style>
    .profile-image {
        width: 150px;
        height: 150px;
        border-radius: 50%;
        float: left;
        margin-right: 40px;
    }
</style>

<img alt="picture 2" src="https://avatars.githubusercontent.com/u/57341225?v=4" class="profile-image" />

<p>
                               [<code>rsikand at stanford.edu</code>]
                               <!-- <br> -->
                               <br>
                               [<a href="https://x.com/rosikand">Twitter</a>] [<a href="https://github.com/rosikand/">Github</a>] [<a href="https://www.linkedin.com/in/rosikand">LinkedIn</a>] [<a href="https://scholar.google.com/citations?user=E5Z8wUoAAAAJ&hl=en">Google Scholar</a>][<a href="https://rosikand.substack.com/">Substack</a>]
                               <br>
                               <br>
                               <a href="https://exhibits.stanford.edu/cs/about/timeline">Stanford CS</a> (AI, Systems) B.S., M.S. 2025.
                           </p>

I am interested in AI research, ML engineering, startups, and venture capital.

In the past, I've conducted research in self-supervised learning at SAIL, engineered segmentation models at insitro, and built many ML projects.

See my [portfolio](./portfolio) page here for past projects and experiences.

I also infrequently write about startups and venture capital on Substack at Rohan's Random Walks.

In my free time, I play [golf](rosikand.github.io/golf), think, and scroll on hackernews. 

I find that the most important forms of learning come from chatting with other people; if you any interests, please feel free to reach out! 


### Current projects 

Currently I am working on two open-source side projects: 

- **MLSys project**: getting large VLA's to run on a Jetson Nano for the purposes of robotic control. The goal here is to learn about model compilation and systems techniques that reduce memory footprint, lowers latency. Robots will become increasingly more integrated within society, but how will we power their intelligence, untethered from the cloud? 
- **LLM from scratch**: the goal is to implement a language model from scratch in JAX. Learning goals include model parallelization, RLHF, kernels, hosting for inference, and more. 


On the **research** side, I am currently interested in scaling inference-time compute to increase reasoning ability of large models, and applying VLM's to data in fields like healthcare and satellite imagery. 

As for **startups/vc**, I love learning about how AI is being brought into production through mediums like the [20VC](https://www.youtube.com/channel/UCf0PBRjhf0rF8fWBIxTuoWA) and No Priors podcasts. 



